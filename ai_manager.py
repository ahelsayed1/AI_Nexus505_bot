# ai_manager.py - Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø± (ÙƒØ§Ù…Ù„ Ø§Ù„Ø®Ø¯Ù…Ø§Øª)
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: 5.1 (Smart Multi-Source AI - Complete) - Ù…Ø¹Ø¯Ù„ Ù„Ù€ Railway
import os
import logging
import asyncio
import google.generativeai as genai
from openai import OpenAI as OpenAIClient
import aiohttp
import re
import json
import base64
from datetime import datetime, timedelta
from enum import Enum
from typing import Optional, List, Dict, Any, Tuple, Union
from dataclasses import dataclass, field
from urllib.parse import urlparse
import hashlib
from collections import OrderedDict
import time

logger = logging.getLogger(__name__)

class ServiceType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"""
    CHAT = "chat"
    IMAGE = "image"
    VIDEO = "video"

class Provider(Enum):
    """Ù…Ø²ÙˆØ¯ÙŠ Ø§Ù„Ø®Ø¯Ù…Ø§Øª"""
    GOOGLE = "google"
    OPENAI = "openai"
    STABILITY = "stability"
    LUMA = "luma"
    KLING = "kling"

@dataclass
class ModelInfo:
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„"""
    name: str
    provider: Provider
    service_type: ServiceType
    version: str = "1.0"
    release_date: Optional[str] = None
    max_tokens: int = 2048
    is_latest: bool = False
    is_deprecated: bool = False
    priority: int = 100
    supports_enhancement: bool = True

@dataclass
class ProviderConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø²ÙˆØ¯ Ø§Ù„Ø®Ø¯Ù…Ø©"""
    name: Provider
    api_key: Optional[str] = None
    enabled: bool = False
    daily_limit: int = 100
    usage_today: int = 0
    errors_today: int = 0
    avg_response_time: float = 0.0
    last_error: Optional[str] = None
    
    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©
    discovered_models: Dict[ServiceType, List[ModelInfo]] = field(default_factory=dict)
    
    # Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù†Ø´Ø· Ø§Ù„Ø­Ø§Ù„ÙŠ
    active_models: Dict[ServiceType, str] = field(default_factory=dict)

class SmartAIManager:
    """
    Ù…Ø¯ÙŠØ± Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø°ÙƒÙŠ ÙŠÙƒØªØ´Ù Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
    ÙˆÙŠØ±ØªØ¨Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ø­Ø¯Ø« Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù‚Ø¯Ù… Ù…Ø¹ Ø¯Ø¹Ù… ÙƒØ§Ù…Ù„ Ù„Ù„Ø®Ø¯Ù…Ø§Øª
    """
    
    def __init__(self, db):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¯ÙŠØ± Ø§Ù„Ø°ÙƒÙŠ"""
        self.db = db
        self.user_limits_cache = OrderedDict()
        self.max_cache_size = 1000
        
        # ØªØ®Ø²ÙŠÙ† Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ù…Ø¹ ÙˆÙ‚Øª Ø§Ù†ØªÙ‡Ø§Ø¡
        self.chat_sessions: Dict[int, Dict[str, Any]] = {}
        self.session_timeout = timedelta(hours=1)
        
        # Ø°Ø§ÙƒØ±Ø© Ù…Ø¤Ù‚ØªØ© Ù„Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©
        self.generated_files_cache: Dict[str, Dict] = {}
        
        # ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ†
        self.providers: Dict[Provider, ProviderConfig] = self._init_providers()
        
        # Ø¹Ù„Ø§Ù…Ø© Ù„Ù„Ø§ÙƒØªØ´Ø§Ù
        self.discovery_completed = False
        self.discovery_lock = asyncio.Lock()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù€ timeout Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        self.default_timeout = aiohttp.ClientTimeout(total=30)
        
        logger.info("ğŸš€ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (ÙƒØ§Ù…Ù„ Ø§Ù„Ø®Ø¯Ù…Ø§Øª)")
    
    def _init_providers(self) -> Dict[Provider, ProviderConfig]:
        """ØªÙ‡ÙŠØ¦Ø© Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ†"""
        providers = {
            Provider.GOOGLE: ProviderConfig(
                name=Provider.GOOGLE,
                api_key=os.getenv("GOOGLE_AI_API_KEY"),
                daily_limit=int(os.getenv("GOOGLE_DAILY_LIMIT", "50"))  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            ),
            Provider.OPENAI: ProviderConfig(
                name=Provider.OPENAI,
                api_key=os.getenv("OPENAI_API_KEY"),
                daily_limit=int(os.getenv("OPENAI_DAILY_LIMIT", "30"))
            ),
            Provider.STABILITY: ProviderConfig(
                name=Provider.STABILITY,
                api_key=os.getenv("STABILITY_API_KEY"),
                daily_limit=int(os.getenv("STABILITY_DAILY_LIMIT", "20"))
            ),
            Provider.LUMA: ProviderConfig(
                name=Provider.LUMA,
                api_key=os.getenv("LUMAAI_API_KEY"),
                daily_limit=int(os.getenv("LUMA_DAILY_LIMIT", "10"))
            ),
            Provider.KLING: ProviderConfig(
                name=Provider.KLING,
                api_key=os.getenv("KLING_API_KEY"),
                daily_limit=int(os.getenv("KLING_DAILY_LIMIT", "5"))
            )
        }
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© API Keys
        for provider_name, config in providers.items():
            if not config.api_key or config.api_key.strip() == "":
                config.enabled = False
                logger.warning(f"âš ï¸ {provider_name.value}: Ù…ÙØªØ§Ø­ API ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ ÙØ§Ø±Øº")
            elif len(config.api_key) < 10:
                config.enabled = False
                logger.warning(f"âš ï¸ {provider_name.value}: Ù…ÙØªØ§Ø­ API ØºÙŠØ± ØµØ§Ù„Ø­ (Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹)")
        
        return providers
    
    async def ensure_discovery(self):
        """Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø§ÙƒØªØ´Ø§Ù ØªÙ…"""
        if not self.discovery_completed:
            async with self.discovery_lock:
                if not self.discovery_completed:
                    await self._setup_and_discover_async()
                    self.discovery_completed = True
    
    async def _setup_and_discover_async(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ APIs ÙˆØ§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†"""
        try:
            logger.info("ğŸ” Ø¨Ø¯Ø¡ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹...")
            
            # 1. Ø¥Ø¹Ø¯Ø§Ø¯ Google API ÙˆØ§ÙƒØªØ´Ø§Ù Ù…ÙˆØ¯ÙŠÙ„Ø§ØªÙ‡
            await self._setup_and_discover_google()
            
            # 2. Ø¥Ø¹Ø¯Ø§Ø¯ OpenAI API ÙˆØ§ÙƒØªØ´Ø§Ù Ù…ÙˆØ¯ÙŠÙ„Ø§ØªÙ‡
            await self._setup_and_discover_openai()
            
            # 3. Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨Ø§Ù‚ÙŠ APIs
            await self._setup_other_apis()
            
            # 4. ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            self._log_discovery_results()
            
            # 5. Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù…ÙˆØ¯ÙŠÙ„ Ù„ÙƒÙ„ Ø®Ø¯Ù…Ø©
            self._select_best_models()
            
            logger.info("âœ… Ø§ÙƒØªÙ…Ù„ Ø§ÙƒØªØ´Ø§Ù ÙˆØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª")
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª: {e}", exc_info=True)
    
    async def _setup_and_discover_google(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Google API ÙˆØ§ÙƒØªØ´Ø§Ù Ù…ÙˆØ¯ÙŠÙ„Ø§ØªÙ‡"""
        google_config = self.providers[Provider.GOOGLE]
        
        if not google_config.api_key:
            logger.warning("âš ï¸ Google API Key ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            return
        
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…ÙØªØ§Ø­
            if not google_config.api_key.startswith("AI"):
                logger.warning("âš ï¸ Google API Key Ù„Ø§ ÙŠØ¨Ø¯Ùˆ Ø¨ØµÙŠØºØ© ØµØ­ÙŠØ­Ø©")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ API
            genai.configure(api_key=google_config.api_key)
            google_config.enabled = True
            
            # Ø§ÙƒØªØ´Ø§Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
            logger.info("ğŸ” Ø¬Ø§Ø±ÙŠ Ø§ÙƒØªØ´Ø§Ù Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Google...")
            all_models = []
            
            try:
                models_list = genai.list_models()
                all_models = [m.name.replace('models/', '') for m in models_list]
                logger.info(f"ğŸ“Š ÙˆØ¬Ø¯ {len(all_models)} Ù…ÙˆØ¯ÙŠÙ„ Google")
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª: {e}")
                # Ù‚Ø§Ø¦Ù…Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© (Ù…Ø¹ Ø§Ù„Ù…ÙˆØ²Ø© Ø£ÙˆÙ„Ø§Ù‹)
                all_models = [
                    'nano-banana-pro-preview',  # 1
                    'imagen-4.0-generate-preview-06-06',  # 2
                    'imagen-3.0-generate-001',  # 3
                    'gemini-2.5-flash', 'gemini-2.5-pro', 'gemini-3-flash-preview',
                    'gemini-3-pro-preview', 'gemini-2.0-flash',
                    'veo-3.0-generate-001'
                ]
            
            # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª
            google_config.discovered_models = {
                ServiceType.CHAT: [],
                ServiceType.IMAGE: [],
                ServiceType.VIDEO: []
            }
            
            for model_name in all_models:
                model_info = self._analyze_google_model(model_name)
                if model_info:
                    google_config.discovered_models[model_info.service_type].append(model_info)
                # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª (Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø£Ù‚Ù„ Ø±Ù‚Ù… priority)
            for service_type in google_config.discovered_models:
                google_config.discovered_models[service_type].sort(
                    key=lambda x: x.priority, # ØªØ±ØªÙŠØ¨ ØªØµØ§Ø¹Ø¯ÙŠ (10 Ø«Ù… 20 Ø«Ù… 50...)
                    reverse=False 
                )
                
            # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø®ØªØ§Ø± Ù„Ù„ØªØ£ÙƒØ¯
            if google_config.discovered_models[ServiceType.CHAT]:
                top_model = google_config.discovered_models[ServiceType.CHAT][0]
                logger.info(f"ğŸ‘‘ ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù‚Ù…Ø©: {top_model.name} (Priority: {top_model.priority})")
                google_config.active_models[ServiceType.CHAT] = top_model.name
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ Ø¥Ø¹Ø¯Ø§Ø¯ Google: {e}", exc_info=True)
            google_config.enabled = False

    def _analyze_google_model(self, model_name: str) -> Optional[ModelInfo]:
        """ØªØ­Ù„ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Google (ÙŠØ¯Ø¹Ù… ØµÙŠØº Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ø®ØªÙ„ÙØ© . Ùˆ -)"""
        try:
            model_lower = model_name.lower()
            
            # Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª
            
            # ØªØµÙ†ÙŠÙ Ø§Ù„Ø®Ø¯Ù…Ø©
            if 'gemini' in model_lower and 'tts' not in model_lower:
                service_type = ServiceType.CHAT
            elif 'imagen' in model_lower or 'banana' in model_lower:
                service_type = ServiceType.IMAGE
            elif 'veo' in model_lower:
                service_type = ServiceType.VIDEO
            else:
                return None
            
            # ====================================================
            # âš¡ï¸ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª (Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ù†Ù‚Ø·Ø© ÙˆØ§Ù„Ø´Ø±Ø·Ø©) âš¡ï¸
            # ====================================================
            version = "1.0"
            priority = 100 
            
            if service_type == ServiceType.CHAT:
                # 1. Gemini 3.0 (ÙŠÙ„ØªÙ‚Ø· 3.0 Ùˆ 3-0 Ùˆ 3 ÙÙ‚Ø·)
                if 'gemini-3' in model_lower:
                    version = "3.0"
                    priority = 10 
                
                # 2. Gemini 2.5 (ÙŠÙ„ØªÙ‚Ø· 2.5 Ùˆ 2-5)
                elif 'gemini-2.5' in model_lower or 'gemini-2-5' in model_lower:
                    version = "2.5"
                    priority = 15
                
                # 3. Gemini 2.0 (ÙŠÙ„ØªÙ‚Ø· 2.0 Ùˆ 2-0)
                elif 'gemini-2.0' in model_lower or 'gemini-2-0' in model_lower:
                    version = "2.0"
                    priority = 20
                
                # 4. Gemini 1.5 (ÙŠÙ„ØªÙ‚Ø· 1.5 Ùˆ 1-5)
                elif 'gemini-1.5' in model_lower or 'gemini-1-5' in model_lower:
                    version = "1.5"
                    priority = 30
                
                # 5. Gemini 1.0
                elif 'gemini-1.0' in model_lower or 'gemini-1-0' in model_lower or 'gemini-pro' in model_lower:
                    version = "1.0"
                    priority = 40
                
            elif service_type == ServiceType.IMAGE:
                # ØªØ±ØªÙŠØ¨ Ø§Ù„ØµÙˆØ±
                if 'banana' in model_lower:      priority = 5
                elif 'imagen-4' in model_lower:  priority = 10
                elif 'imagen-3' in model_lower:  priority = 20
                elif 'imagen-2' in model_lower:  priority = 30
                else: priority = 50
                
            elif service_type == ServiceType.VIDEO:
                if 'veo' in model_lower: priority = 10
            
            return ModelInfo(
                name=model_name, provider=Provider.GOOGLE, service_type=service_type, 
                version=version, is_latest=('latest' in model_lower), priority=priority
            )
            
        except: return None
    
    async def _setup_and_discover_openai(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ OpenAI API ÙˆØ§ÙƒØªØ´Ø§Ù Ù…ÙˆØ¯ÙŠÙ„Ø§ØªÙ‡"""
        openai_config = self.providers[Provider.OPENAI]
        
        if not openai_config.api_key:
            logger.warning("âš ï¸ OpenAI API Key ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            return
        
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…ÙØªØ§Ø­
            if not openai_config.api_key.startswith("sk-"):
                logger.warning("âš ï¸ OpenAI API Key Ù„Ø§ ÙŠØ¨Ø¯Ùˆ Ø¨ØµÙŠØºØ© ØµØ­ÙŠØ­Ø© (ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¨Ø¯Ø£ Ø¨Ù€ sk-)")
            
            self.openai_client = OpenAIClient(api_key=openai_config.api_key)
            openai_config.enabled = True
            
            logger.info("ğŸ” Ø¬Ø§Ø±ÙŠ Ø§ÙƒØªØ´Ø§Ù Ù…ÙˆØ¯ÙŠÙ„Ø§Øª OpenAI...")
            
            openai_config.discovered_models = {
                ServiceType.CHAT: [],
                ServiceType.IMAGE: [],
                ServiceType.VIDEO: []
            }
            
            # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©
            known_openai_models = {
                ServiceType.CHAT: [
                    'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4', 
                    'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct'
                ],
                ServiceType.IMAGE: [
                    'dall-e-3', 'dall-e-2'
                ]
            }
            
            # Ø§Ø®ØªØ¨Ø§Ø± ÙƒÙ„ Ù…ÙˆØ¯ÙŠÙ„
            for service_type, models in known_openai_models.items():
                for model_name in models:
                    model_info = self._analyze_openai_model(model_name, service_type)
                    if model_info:
                        openai_config.discovered_models[service_type].append(model_info)
            
            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª
            for service_type in openai_config.discovered_models:
                openai_config.discovered_models[service_type].sort(key=lambda x: x.priority)
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ Ø¥Ø¹Ø¯Ø§Ø¯ OpenAI: {e}", exc_info=True)
            openai_config.enabled = False
    
    def _analyze_openai_model(self, model_name: str, service_type: ServiceType) -> Optional[ModelInfo]:
        """ØªØ­Ù„ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ OpenAI"""
        try:
            model_lower = model_name.lower()
            version = "1.0"
            priority = 100
            
            if service_type == ServiceType.CHAT:
                if 'gpt-4o' in model_lower:
                    version = "4.0"
                    priority = 5 if 'mini' in model_lower else 10
                elif 'gpt-4-turbo' in model_lower:
                    version = "4.0"
                    priority = 15
                elif 'gpt-4' in model_lower:
                    version = "4.0"
                    priority = 20
                elif 'gpt-3.5-turbo' in model_lower:
                    version = "3.5"
                    priority = 30 if 'instruct' in model_lower else 25
                else:
                    return None
                    
            elif service_type == ServiceType.IMAGE:
                if 'dall-e-3' in model_lower:
                    version = "3.0"
                    priority = 5
                elif 'dall-e-2' in model_lower:
                    version = "2.0"
                    priority = 10
                else:
                    return None
            
            return ModelInfo(
                name=model_name,
                provider=Provider.OPENAI,
                service_type=service_type,
                version=version,
                priority=priority,
                supports_enhancement=True
            )
            
        except Exception as e:
            logger.debug(f"âš ï¸ ÙØ´Ù„ ØªØ­Ù„ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ OpenAI {model_name}: {e}")
            return None
    
    async def _setup_other_apis(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨Ø§Ù‚ÙŠ APIs"""
        # Stability AI
        stability_config = self.providers[Provider.STABILITY]
        if stability_config.api_key:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…ÙØªØ§Ø­
            if len(stability_config.api_key) > 20:
                stability_config.enabled = True
                self.stability_headers = {
                    "Authorization": f"Bearer {stability_config.api_key}",
                    "Content-Type": "application/json",
                    "Accept": "application/json",
                    "User-Agent": "SmartAIManager/5.1"
                }
                self.stability_url = os.getenv(
                    "STABLE_DIFFUSION_URL", 
                    "https://api.stability.ai/v1/generation/stable-diffusion-xl-1024-v1-0/text-to-image"
                )
        
        # Luma AI
        luma_config = self.providers[Provider.LUMA]
        if luma_config.api_key:
            if len(luma_config.api_key) > 20:
                luma_config.enabled = True
                self.luma_headers = {
                    "Authorization": f"Bearer {luma_config.api_key}",
                    "Content-Type": "application/json",
                    "User-Agent": "SmartAIManager/5.1"
                }
        
        # Kling AI
        kling_config = self.providers[Provider.KLING]
        if kling_config.api_key:
            if len(kling_config.api_key) > 20:
                kling_config.enabled = True
                self.kling_headers = {
                    "Authorization": f"Bearer {kling_config.api_key}",
                    "Content-Type": "application/json",
                    "User-Agent": "SmartAIManager/5.1"
                }
    
    def _log_discovery_results(self):
        """ØªØ³Ø¬ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§ÙƒØªØ´Ø§Ù (Ø¹Ø±Ø¶ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ÙƒØ§Ù…Ù„Ø©)"""
        logger.info("=" * 50)
        logger.info("ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª (Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©):")
        
        for provider_name, config in self.providers.items():
            if not config.enabled:
                continue
            
            logger.info(f"\nğŸ”¹ {provider_name.value.upper()}:")
            
            for service_type, models in config.discovered_models.items():
                if models:
                    logger.info(f"  {service_type.value} ({len(models)} models):")
                    for i, model in enumerate(models):
                        # âœ… ØªØ¹Ø¯ÙŠÙ„: ÙˆØ¶Ø¹ Ù†Ø¬Ù…Ø© Ù„Ø£ÙˆÙ„ 16 Ù…ÙˆØ¯ÙŠÙ„ (Ù„Ø£Ù†Ù†Ø§ Ø³Ù†Ø­Ø§ÙˆÙ„ 16 Ù…Ø±Ø©)
                        status = "â­" if i < 16 else "  "
                        logger.info(f"    {status} [{i+1}] {model.name} (Priority: {model.priority})")
                else:
                    logger.info(f"  {service_type.value}: âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙˆØ¯ÙŠÙ„Ø§Øª")
        
        logger.info("=" * 50)
    
    def _select_best_models(self):
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù…ÙˆØ¯ÙŠÙ„ Ù„ÙƒÙ„ Ø®Ø¯Ù…Ø©"""
        for provider_name, config in self.providers.items():
            if not config.enabled:
                continue
            
            for service_type, models in config.discovered_models.items():
                if models:
                    best_model = min(models, key=lambda x: x.priority)
                    config.active_models[service_type] = best_model.name
    
    def _extract_version_number(self, version_str: str) -> float:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ù‚Ù… Ø§Ù„Ø¥ØµØ¯Ø§Ø±"""
        try:
            return float(version_str)
        except:
            return 1.0
    
    # ==================== Ø¯ÙˆØ§Ù„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ ====================
    
    def get_available_providers(self, service_type: ServiceType) -> List[ProviderConfig]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ† Ø§Ù„Ù…ØªØ§Ø­ÙŠÙ†"""
        available = []
        
        for provider in self.providers.values():
            if not provider.enabled or provider.usage_today >= provider.daily_limit:
                continue
            
            if service_type == ServiceType.CHAT:
                if provider.name in [Provider.GOOGLE, Provider.OPENAI]:
                    available.append(provider)
            elif service_type == ServiceType.IMAGE:
                if provider.name in [Provider.GOOGLE, Provider.OPENAI, Provider.STABILITY]:
                    available.append(provider)
            elif service_type == ServiceType.VIDEO:
                if provider.name in [Provider.GOOGLE, Provider.LUMA, Provider.KLING]:
                    available.append(provider)
        
        available.sort(key=lambda x: (x.errors_today, x.usage_today))
        return available
    
    def get_active_model(self, provider: Provider, service_type: ServiceType) -> Optional[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù†Ø´Ø·"""
        config = self.providers.get(provider)
        if not config or not config.enabled:
            return None
        
        return config.active_models.get(service_type)
    
    def rotate_model(self, provider: Provider, service_type: ServiceType, current_model: str = None) -> Optional[str]:
        """ØªØ¯ÙˆÙŠØ± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¥Ù„Ù‰ Ø§Ù„ØªØ§Ù„ÙŠ"""
        config = self.providers.get(provider)
        if not config or service_type not in config.discovered_models:
            return None
        
        models = config.discovered_models[service_type]
        if not models:
            return None
        
        if not current_model or current_model not in [m.name for m in models]:
            new_model = models[0].name
        else:
            current_index = None
            for i, model in enumerate(models):
                if model.name == current_model:
                    current_index = i
                    break
            
            if current_index is None or current_index >= len(models) - 1:
                new_model = models[0].name
            else:
                new_model = models[current_index + 1].name
        
        config.active_models[service_type] = new_model
        logger.info(f"ğŸ”„ ØªØ¯ÙˆÙŠØ± Ù…ÙˆØ¯ÙŠÙ„ {provider.value}/{service_type.value}: {current_model or 'None'} â†’ {new_model}")
        return new_model
    
    async def _execute_with_fallback(self, provider: Provider, service_type: ServiceType, 
                                   execute_func, max_retries: int = 3):
        """ØªÙ†ÙÙŠØ° Ù…Ø¹ Ù†Ø¸Ø§Ù… fallback"""
        config = self.providers.get(provider)
        if not config or not config.enabled:
            raise Exception(f"Ø§Ù„Ù…Ø²ÙˆØ¯ {provider.value} ØºÙŠØ± Ù…ÙØ¹Ù„")
        
        current_model = self.get_active_model(provider, service_type)
        original_model = current_model
        
        for attempt in range(max_retries):
            try:
                if not current_model:
                    models = config.discovered_models.get(service_type, [])
                    if not models:
                        raise Exception(f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ù„Ù€ {service_type.value}")
                    current_model = models[0].name
                    config.active_models[service_type] = current_model
                
                logger.info(f"ğŸ”„ Ù…Ø­Ø§ÙˆÙ„Ø© {attempt+1}/{max_retries} Ù…Ø¹ {provider.value}/{current_model}")
                
                result = await execute_func(current_model)
                return result
                
            except Exception as e:
                error_msg = str(e)
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ {provider.value}/{current_model}: {error_msg}")
                
                is_quota_error = any(keyword in error_msg.lower() for keyword in [
                    '429', 'quota', 'rate limit', 'resource exhausted'
                ])
                is_model_error = any(keyword in error_msg.lower() for keyword in [
                    '404', 'not found', 'invalid model', 'model not found'
                ])
                
                if is_quota_error or is_model_error:
                    logger.warning(f"âš ï¸ {provider.value}: Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ {current_model}")
                    
                    next_model = self.rotate_model(provider, service_type, current_model)
                    
                    if next_model and next_model != current_model:
                        current_model = next_model
                        logger.info(f"ğŸ”„ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„ØªØ§Ù„ÙŠ: {next_model}")
                        continue
                    else:
                        logger.error(f"âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø¨Ø¯ÙŠÙ„Ø© Ù„Ù€ {provider.value}")
                        break
                else:
                    logger.error(f"âŒ {provider.value}: Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„")
                    break
        
        raise Exception(f"ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø§ÙˆÙ„Ø§Øª {provider.value} ({max_retries} Ù…Ø­Ø§ÙˆÙ„Ø§Øª)")
    
    # ==================== Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (ÙƒØ§Ù…Ù„Ø©) ====================
    
    async def chat_with_ai(self, user_id: int, message: str) -> str:
        """Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ fallback Ø°ÙƒÙŠ"""
        try:
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø§ÙƒØªØ´Ø§Ù ØªÙ…
            await self.ensure_discovery()
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            if not isinstance(user_id, int) or user_id <= 0:
                return "âŒ Ù…Ø¹Ø±Ù Ù…Ø³ØªØ®Ø¯Ù… ØºÙŠØ± ØµØ§Ù„Ø­."
            
            if not message or len(message.strip()) < 1:
                return "âŒ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙØ§Ø±ØºØ© Ø£Ùˆ Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹."
            
            if len(message) > 4000:
                message = message[:4000] + "..."
            
            allowed, remaining = self.check_user_limit(user_id, "ai_chat")
            if not allowed:
                return f"âŒ Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù‚Ø¯ Ø§Ø³ØªÙ‡Ù„ÙƒØª Ø±ØµÙŠØ¯Ùƒ Ø§Ù„ÙŠÙˆÙ…ÙŠ Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ø¦Ù„. ({remaining} Ù…ØªØ¨Ù‚ÙŠ)"
            
            providers = self.get_available_providers(ServiceType.CHAT)
            
            if not providers:
                return "âš ï¸ Ø¬Ù…ÙŠØ¹ Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹."
            
            errors = []
            
            for provider_config in providers:
                try:
                    provider = provider_config.name
                    
                    async def execute_chat(model_name: str):
                        if provider == Provider.GOOGLE:
                            return await self._chat_with_google(model_name, user_id, message)
                        elif provider == Provider.OPENAI:
                            return await self._chat_with_openai(model_name, message)
                        else:
                            raise Exception(f"Ù…Ø²ÙˆØ¯ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {provider}")
                    
                    # âœ… Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¥Ù„Ù‰ 16 (ÙƒÙ…Ø§ Ø·Ù„Ø¨Øª)
                    response = await self._execute_with_fallback(
                        provider, ServiceType.CHAT, execute_chat, max_retries=16
                    )
                    
                    if response:
                        self.update_user_usage(user_id, "ai_chat")
                        provider_config.usage_today += 1
                        self.db.save_ai_conversation(user_id, "chat", message, response)
                        return response
                        
                except Exception as e:
                    error_msg = str(e)
                    errors.append(f"{provider_config.name.value}: {error_msg[:100]}")
                    provider_config.errors_today += 1
                    provider_config.last_error = error_msg
                    continue
            
            if errors:
                error_summary = "\n".join(errors[:3])
                return f"âš ï¸ Ø¬Ù…ÙŠØ¹ Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙØ´Ù„Øª:\n{error_summary}"
            return "âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹."
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {e}", exc_info=True)
            return "âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…."
    
    async def _chat_with_google(self, model_name: str, user_id: int, message: str) -> str:
        """Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ù…Ø¹ Google Gemini"""
        try:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            self._cleanup_old_sessions()
            
            model = genai.GenerativeModel(model_name)
            
            if user_id not in self.chat_sessions:
                chat = model.start_chat(history=[
                    {"role": "user", "parts": ["Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ø±Ø¯ Ø¨Ø§Ø®ØªØµØ§Ø± ÙˆÙˆØ¶ÙˆØ­."]},
                    {"role": "model", "parts": ["Ø­Ø³Ù†Ø§Ù‹ØŒ Ø£Ù†Ø§ Ø¬Ø§Ù‡Ø² Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©."]}
                ])
                self.chat_sessions[user_id] = {
                    "chat": chat,
                    "last_activity": datetime.now()
                }
            
            session = self.chat_sessions[user_id]
            session["last_activity"] = datetime.now()
            chat_session = session["chat"]
            
            response = await asyncio.wait_for(
                chat_session.send_message_async(message),
                timeout=30.0
            )
            
            if response and response.text:
                return self._clean_response(response.text)
            else:
                raise Exception("Ø±Ø¯ ÙØ§Ø±Øº Ù…Ù† Google")
                
        except asyncio.TimeoutError:
            raise Exception("Ø§Ù†ØªÙ‡Ù‰ ÙˆÙ‚Øª Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù„Ø±Ø¯ Ù…Ù† Google")
        except Exception as e:
            raise Exception(f"Google Gemini error: {str(e)}")
    
    async def _chat_with_openai(self, model_name: str, message: str) -> str:
        """Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ù…Ø¹ OpenAI"""
        try:
            response = await asyncio.wait_for(
                asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.openai_client.chat.completions.create(
                        model=model_name,
                        messages=[{"role": "user", "content": message}],
                        max_tokens=1000,
                        temperature=0.7
                    )
                ),
                timeout=30.0
            )
            return response.choices[0].message.content
        except asyncio.TimeoutError:
            raise Exception("Ø§Ù†ØªÙ‡Ù‰ ÙˆÙ‚Øª Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù„Ø±Ø¯ Ù…Ù† OpenAI")
        except Exception as e:
            raise Exception(f"OpenAI error: {str(e)}")
    
    def _cleanup_old_sessions(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
        now = datetime.now()
        to_delete = []
        
        for user_id, session_data in self.chat_sessions.items():
            if now - session_data["last_activity"] > self.session_timeout:
                to_delete.append(user_id)
        
        for user_id in to_delete:
            del self.chat_sessions[user_id]
        
        if to_delete:
            logger.info(f"ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ {len(to_delete)} Ø¬Ù„Ø³Ø© Ù‚Ø¯ÙŠÙ…Ø©")
    
    # ==================== Ø®Ø¯Ù…Ø© Ø§Ù„ØµÙˆØ± (ÙƒØ§Ù…Ù„Ø©) ====================
    
    async def generate_image(self, user_id: int, prompt: str, style: str = "realistic") -> Tuple[Optional[str], str]:
        """ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ù…Ø¹ fallback Ø°ÙƒÙŠ"""
        try:
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø§ÙƒØªØ´Ø§Ù ØªÙ…
            await self.ensure_discovery()
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            if not isinstance(user_id, int) or user_id <= 0:
                return None, "âŒ Ù…Ø¹Ø±Ù Ù…Ø³ØªØ®Ø¯Ù… ØºÙŠØ± ØµØ§Ù„Ø­."
            
            if not prompt or len(prompt.strip()) < 3:
                return None, "âŒ Ø§Ù„ÙˆØµÙ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ (Ø£Ù‚Ù„ Ù…Ù† 3 Ø£Ø­Ø±Ù)."
            
            if len(prompt) > 2000:
                prompt = prompt[:2000]
            
            allowed, remaining = self.check_user_limit(user_id, "image_gen")
            if not allowed:
                return None, f"âŒ Ø§Ù†ØªÙ‡Ù‰ Ø±ØµÙŠØ¯ Ø§Ù„ØµÙˆØ± Ø§Ù„ÙŠÙˆÙ…ÙŠ. ({remaining} Ù…ØªØ¨Ù‚ÙŠ)"
            
            providers = self.get_available_providers(ServiceType.IMAGE)
            
            if not providers:
                return None, "âš ï¸ Ø¬Ù…ÙŠØ¹ Ø®Ø¯Ù…Ø§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± ØºÙŠØ± Ù…ØªØ§Ø­Ø©."
            
            errors = []
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØµÙ
            enhanced_prompt = await self._enhance_image_prompt(prompt, style)
            
            for provider_config in providers:
                try:
                    provider = provider_config.name
                    
                    async def execute_image(model_name: str):
                        if provider == Provider.GOOGLE:
                            return await self._generate_image_google(model_name, enhanced_prompt)
                        elif provider == Provider.OPENAI:
                            return await self._generate_image_openai(model_name, enhanced_prompt)
                        elif provider == Provider.STABILITY:
                            return await self._generate_image_stability(enhanced_prompt, style)
                        else:
                            raise Exception(f"Ù…Ø²ÙˆØ¯ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ… Ù„Ù„ØµÙˆØ±: {provider}")
                    
                    # âœ… Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¥Ù„Ù‰ 6
                    image_url = await self._execute_with_fallback(
                        provider, ServiceType.IMAGE, execute_image, max_retries=6
                    )
                    
                    if image_url:
                        self.update_user_usage(user_id, "image_gen")
                        provider_config.usage_today += 1
                        self.db.save_generated_file(user_id, "image", prompt, image_url)
                        
                        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´
                        cache_key = f"image_{user_id}_{hashlib.md5(prompt.encode()).hexdigest()[:12]}"
                        self.generated_files_cache[cache_key] = {
                            "url": image_url,
                            "prompt": prompt,
                            "provider": provider.value,
                            "timestamp": datetime.now().isoformat()
                        }
                        
                        return image_url, "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù†Ø¬Ø§Ø­"
                        
                except Exception as e:
                    error_msg = str(e)
                    errors.append(f"{provider_config.name.value}: {error_msg[:100]}")
                    provider_config.errors_today += 1
                    provider_config.last_error = error_msg
                    continue
            
            if errors:
                error_summary = "\n".join(errors[:3])
                return None, f"âŒ Ø¬Ù…ÙŠØ¹ Ø®Ø¯Ù…Ø§Øª Ø§Ù„ØµÙˆØ± ÙØ´Ù„Øª:\n{error_summary}"
            return None, "âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹."
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±: {e}", exc_info=True)
            return None, "âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„ØµÙˆØ±."
    
    async def _enhance_image_prompt(self, prompt: str, style: str) -> str:
        """ØªØ­Ø³ÙŠÙ† ÙˆØµÙ Ø§Ù„ØµÙˆØ±Ø©"""
        style_map = {
            "realistic": "ÙÙˆØªÙˆØºØ±Ø§ÙÙŠ ÙˆØ§Ù‚Ø¹ÙŠØŒ ØªÙØ§ØµÙŠÙ„ Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø¥Ø¶Ø§Ø¡Ø© Ø·Ø¨ÙŠØ¹ÙŠØ©",
            "anime": "Ø£Ù†Ù…ÙŠ ÙŠØ§Ø¨Ø§Ù†ÙŠØŒ Ø£Ù„ÙˆØ§Ù† Ø²Ø§Ù‡ÙŠØ©ØŒ Ø¹ÙŠÙˆÙ† ÙƒØ¨ÙŠØ±Ø©",
            "fantasy": "ÙØ§Ù†ØªØ§Ø²ÙŠØ§ Ø³Ø­Ø±ÙŠØ©ØŒ ÙƒØ§Ø¦Ù†Ø§Øª Ø®ÙŠØ§Ù„ÙŠØ©ØŒ Ø¥Ø¶Ø§Ø¡Ø© Ø¯Ø±Ø§Ù…Ø§ØªÙŠÙƒÙŠØ©",
            "cyberpunk": "Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØŒ Ù†ÙŠÙˆÙ†ØŒ ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ù…ØªÙ‚Ø¯Ù…Ø©",
            "watercolor": "Ø£Ù„ÙˆØ§Ù† Ù…Ø§Ø¦ÙŠØ©ØŒ ÙØ±Ø´Ø§Ø© ÙÙ†ÙŠØ©ØŒ Ø§Ù†Ø³ÙŠØ§Ø¨ÙŠØ©"
        }
        
        style_desc = style_map.get(style, "ÙÙˆØªÙˆØºØ±Ø§ÙÙŠ ÙˆØ§Ù‚Ø¹ÙŠ")
        
        enhancement_prompt = f"""
        Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„ÙˆØµÙ Ù„ØµÙˆØ±Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©:
        Ø§Ù„ÙˆØµÙ: {prompt}
        Ø§Ù„Ù†Ù…Ø·: {style} ({style_desc})
        
        Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª:
        1. ÙˆØµÙ Ù…ÙØµÙ„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        2. Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© ÙˆØ§Ù„ØªØ±ÙƒÙŠØ¨
        3. Ø¥Ø¶Ø§ÙØ© ØªÙØ§ØµÙŠÙ„ ÙÙ†ÙŠØ©
        4. Ù…Ù†Ø§Ø³Ø¨ Ù„ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± AI
        
        Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: ÙˆØµÙ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ ÙÙ‚Ø·
        """
        
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Gemini Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØµÙ
            google_config = self.providers[Provider.GOOGLE]
            if google_config.enabled and google_config.active_models.get(ServiceType.CHAT):
                model_name = google_config.active_models[ServiceType.CHAT]
                model = genai.GenerativeModel(model_name)
                response = await asyncio.wait_for(
                    model.generate_content_async(enhancement_prompt),
                    timeout=15.0
                )
                if response and response.text:
                    enhanced = response.text.strip()
                    if len(enhanced) > 50:  # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø­ØªÙˆÙ‰ ÙƒØ§ÙÙŠ
                        return enhanced
        except Exception as e:
            logger.debug(f"âš ï¸ ÙØ´Ù„ ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØµÙ: {e}")
        
        return f"{prompt}, {style} style, professional photography, detailed, 4k"
    
    async def _generate_image_google(self, model_name: str, prompt: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© (ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„)"""
        try:
            logger.info(f"ğŸ¨ Ù…Ø­Ø§ÙˆÙ„Ø© ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…: {model_name}...")
            
            if not os.path.exists("downloads"):
                os.makedirs("downloads")
            filename = f"downloads/img_{int(time.time())}.png"

            api_key = self.providers[Provider.GOOGLE].api_key
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ± (Ø§Ù„Ø°ÙŠ ÙŠØ­Ø¯Ø¯Ù‡ Ù†Ø¸Ø§Ù… Ø§Ù„Ù€ Fallback)
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:predict?key={api_key}"
            
            payload = {
                "instances": [{"prompt": prompt}],
                "parameters": {"sampleCount": 1, "aspectRatio": "1:1"}
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload, headers={'Content-Type': 'application/json'}) as response:
                    # Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ (Ù…Ø«Ù„Ø§Ù‹ 404 Ù„Ù„Ù…ÙˆØ²Ø©)ØŒ Ù†Ø±ÙØ¹ Ø®Ø·Ø£ Ù„ÙŠØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡
                    if response.status != 200:
                        error_text = await response.text()
                        # Ù†Ø±ÙØ¹ Exception ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 404 Ù„ÙŠÙÙ‡Ù… Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆÙŠØ¬Ø±Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„ØªØ§Ù„ÙŠ
                        raise Exception(f"Google Error {response.status}: {error_text}")
                    
                    result = await response.json()
                    
                    predictions = result.get('predictions', [])
                    if not predictions:
                        raise Exception("Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªÙ„Ø§Ù… ØªÙ†Ø¨Ø¤Ø§Øª (Empty Response)")
                    
                    b64_data = predictions[0].get('bytesBase64Encoded')
                    if not b64_data:
                         b64_data = predictions[0].get('image', {}).get('bytesBase64Encoded')
                         
                    if not b64_data:
                        raise Exception("ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØµÙˆØ±Ø© ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ")
                        
                    image_data = base64.b64decode(b64_data)
                    with open(filename, "wb") as f:
                        f.write(image_data)
                        
                    return filename

        except Exception as e:
            # Ù‡Ù†Ø§ Ù„Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù„Ù€ OpenAI ÙÙˆØ±Ø§Ù‹
            # Ø¨Ù„ Ù†ØªØ±Ùƒ Ø§Ù„Ø®Ø·Ø£ ÙŠØµØ¹Ø¯ Ù„ÙƒÙŠ ÙŠÙ‚ÙˆÙ… _execute_with_fallback Ø¨ØªØ¬Ø±Ø¨Ø© Ù…ÙˆØ¯ÙŠÙ„ Ø¬ÙˆØ¬Ù„ Ø§Ù„ØªØ§Ù„ÙŠ
            raise e
    
    async def _generate_image_openai(self, model_name: str, prompt: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI DALL-E"""
        try:
            response = await asyncio.wait_for(
                asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.openai_client.images.generate(
                        model=model_name,
                        prompt=prompt[:1000],
                        size="1024x1024",
                        quality="standard",
                        n=1
                    )
                ),
                timeout=60.0
            )
            return response.data[0].url
        except asyncio.TimeoutError:
            raise Exception("Ø§Ù†ØªÙ‡Ù‰ ÙˆÙ‚Øª Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù€ DALL-E")
        except Exception as e:
            raise Exception(f"DALL-E error: {str(e)}")
    
    async def _generate_image_stability(self, prompt: str, style: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Stability AI"""
        try:
            style_presets = {
                "realistic": "photographic",
                "anime": "anime",
                "fantasy": "fantasy-art",
                "cyberpunk": "neon-punk",
                "watercolor": None  # Ù„Ø§ ÙŠÙˆØ¬Ø¯ preset
            }
            
            data = {
                "text_prompts": [{"text": prompt, "weight": 1}],
                "cfg_scale": 7,
                "height": 1024,
                "width": 1024,
                "samples": 1,
                "steps": 30,
            }
            
            style_preset = style_presets.get(style)
            if style_preset:
                data["style_preset"] = style_preset
            
            async with aiohttp.ClientSession(timeout=self.default_timeout) as session:
                async with session.post(
                    self.stability_url,
                    headers=self.stability_headers,
                    json=data
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        
                        # Stability ØªØ¹ÙŠØ¯ base64
                        if "artifacts" in result and len(result["artifacts"]) > 0:
                            image_data = result["artifacts"][0]["base64"]
                            
                            # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ†Ùƒ:
                            # 1. Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© ÙÙŠ Ù…Ù„Ù Ù…Ø¤Ù‚Øª
                            # 2. Ø±ÙØ¹Ù‡Ø§ Ù„Ø®Ø¯Ù…Ø© ØªØ®Ø²ÙŠÙ†
                            # 3. Ø¥Ø¹Ø§Ø¯Ø© Ø±Ø§Ø¨Ø·Ù‡Ø§
                            
                            # Ù…Ø¤Ù‚ØªØ§Ù‹ Ù†Ø¹ÙŠØ¯ Ø±Ø§Ø¨Ø· ÙˆÙ‡Ù…ÙŠ Ù…Ø¹ ØªÙ†Ø¨ÙŠÙ
                            logger.warning("âš ï¸ Stability AI: ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„ØµÙˆØ±Ø© ÙƒÙ€ base64ØŒ ØªØ­ØªØ§Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø©")
                            return f"data:image/png;base64,{image_data}"
                        else:
                            raise Exception("Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙˆØ± ÙÙŠ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Stability")
                    else:
                        error_text = await response.text()[:200]
                        raise Exception(f"Stability API error: {response.status} - {error_text}")
        except aiohttp.ClientError as e:
            raise Exception(f"Stability AI connection error: {str(e)}")
        except Exception as e:
            raise Exception(f"Stability AI error: {str(e)}")
    
    # ==================== Ø®Ø¯Ù…Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (ÙƒØ§Ù…Ù„Ø©) ====================
    
    async def generate_video(self, user_id: int, prompt: str, image_url: str = None) -> Tuple[Optional[str], str]:
        """ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ø¹ fallback Ø°ÙƒÙŠ"""
        try:
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø§ÙƒØªØ´Ø§Ù ØªÙ…
            await self.ensure_discovery()
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            if not isinstance(user_id, int) or user_id <= 0:
                return None, "âŒ Ù…Ø¹Ø±Ù Ù…Ø³ØªØ®Ø¯Ù… ØºÙŠØ± ØµØ§Ù„Ø­."
            
            if not prompt or len(prompt.strip()) < 5:
                return None, "âŒ Ø§Ù„ÙˆØµÙ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ (Ø£Ù‚Ù„ Ù…Ù† 5 Ø£Ø­Ø±Ù)."
            
            if len(prompt) > 1000:
                prompt = prompt[:1000]
            
            allowed, remaining = self.check_user_limit(user_id, "video_gen")
            if not allowed:
                return None, f"âŒ Ø§Ù†ØªÙ‡Ù‰ Ø±ØµÙŠØ¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠ. ({remaining} Ù…ØªØ¨Ù‚ÙŠ)"
            
            providers = self.get_available_providers(ServiceType.VIDEO)
            
            if not providers:
                return None, "âš ï¸ Ø¬Ù…ÙŠØ¹ Ø®Ø¯Ù…Ø§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ØºÙŠØ± Ù…ØªØ§Ø­Ø©."
            
            errors = []
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØµÙ
            enhanced_prompt = await self._enhance_video_prompt(prompt)
            
            for provider_config in providers:
                try:
                    provider = provider_config.name
                    
                    async def execute_video(model_name: str):
                        if provider == Provider.GOOGLE:
                            return await self._generate_video_google(model_name, enhanced_prompt, image_url)
                        elif provider == Provider.LUMA:
                            return await self._generate_video_luma(enhanced_prompt, image_url)
                        elif provider == Provider.KLING:
                            return await self._generate_video_kling(enhanced_prompt, image_url)
                        else:
                            raise Exception(f"Ù…Ø²ÙˆØ¯ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ… Ù„Ù„ÙÙŠØ¯ÙŠÙˆ: {provider}")
                    
                    # âœ… Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¥Ù„Ù‰ 6
                    video_url = await self._execute_with_fallback(
                        provider, ServiceType.VIDEO, execute_video, max_retries=6
                    )
                    
                    if video_url:
                        self.update_user_usage(user_id, "video_gen")
                        provider_config.usage_today += 1
                        self.db.save_generated_file(user_id, "video", prompt, video_url)
                        
                        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´
                        cache_key = f"video_{user_id}_{hashlib.md5(prompt.encode()).hexdigest()[:12]}"
                        self.generated_files_cache[cache_key] = {
                            "url": video_url,
                            "prompt": prompt,
                            "provider": provider.value,
                            "timestamp": datetime.now().isoformat()
                        }
                        
                        return video_url, "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø¬Ø§Ø­"
                        
                except Exception as e:
                    error_msg = str(e)
                    errors.append(f"{provider_config.name.value}: {error_msg[:100]}")
                    provider_config.errors_today += 1
                    provider_config.last_error = error_msg
                    continue
            
            if errors:
                error_summary = "\n".join(errors[:3])
                return None, f"âŒ Ø¬Ù…ÙŠØ¹ Ø®Ø¯Ù…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙØ´Ù„Øª:\n{error_summary}"
            return None, "âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹."
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {e}", exc_info=True)
            return None, "âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ."
    
    async def _enhance_video_prompt(self, prompt: str) -> str:
        """ØªØ­Ø³ÙŠÙ† ÙˆØµÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
        enhancement_prompt = f"""
        Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„ÙˆØµÙ Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ø­ØªØ±Ø§ÙÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©:
        Ø§Ù„ÙˆØµÙ: {prompt}
        
        Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª:
        1. ÙÙŠØ¯ÙŠÙˆ 5 Ø«ÙˆØ§Ù†ÙŠ
        2. ÙˆØµÙ Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        3. ØªØ­Ø¯ÙŠØ¯ Ø­Ø±ÙƒØ© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ (zoom, pan, tilt)
        4. ÙˆØµÙ Ø§Ù„Ø­Ø±ÙƒØ© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø´Ù‡Ø¯
        5. Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© ÙˆØ§Ù„Ù…Ø²Ø§Ø¬
        6. Ù…Ù†Ø§Ø³Ø¨ Ù„ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ AI
        
        Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: ÙˆØµÙ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ ÙÙ‚Ø·
        """
        
        try:
            google_config = self.providers[Provider.GOOGLE]
            if google_config.enabled and google_config.active_models.get(ServiceType.CHAT):
                model_name = google_config.active_models[ServiceType.CHAT]
                model = genai.GenerativeModel(model_name)
                response = await asyncio.wait_for(
                    model.generate_content_async(enhancement_prompt),
                    timeout=15.0
                )
                if response and response.text:
                    enhanced = response.text.strip()
                    if len(enhanced) > 100:  # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø­ØªÙˆÙ‰ ÙƒØ§ÙÙŠ
                        return enhanced
        except Exception as e:
            logger.debug(f"âš ï¸ ÙØ´Ù„ ØªØ­Ø³ÙŠÙ† ÙˆØµÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {e}")
        
        return f"{prompt}, cinematic, 5 seconds, smooth camera movement, professional lighting"
    
    async def _generate_video_google(self, model_name: str, prompt: str, image_url: str = None) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Veo"""
        # TODO: ØªÙ†ÙÙŠØ° API call Ù„Ù€ Google Veo
        # Ù…Ø¤Ù‚ØªØ§Ù‹ Ù†Ø³ØªØ®Ø¯Ù… fallback Ù„Ù€ Luma Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­
        luma_config = self.providers[Provider.LUMA]
        if luma_config.enabled:
            return await self._generate_video_luma(prompt, image_url)
        raise Exception("Google Veo ØºÙŠØ± Ù…ØªÙˆÙØ± Ø­Ø§Ù„ÙŠØ§Ù‹")
    
    async def _generate_video_luma(self, prompt: str, image_url: str = None) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Luma AI"""
        try:
            url = "https://api.lumalabs.ai/dream-machine/v1/generations"
            payload = {
                "prompt": prompt,
                "aspect_ratio": "16:9"
            }
            
            if image_url:
                url = "https://api.lumalabs.ai/dream-machine/v1/generations/image"
                payload["image_url"] = image_url
            
            timeout = aiohttp.ClientTimeout(total=300)  # 5 Ø¯Ù‚Ø§Ø¦Ù‚ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                # Ø¨Ø¯Ø¡ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
                async with session.post(url, headers=self.luma_headers, json=payload) as response:
                    if response.status in [200, 201]:
                        data = await response.json()
                        generation_id = data.get("id")
                        
                        if not generation_id:
                            raise Exception("Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ù…Ø¹Ø±Ù Ø§Ù„ØªÙˆÙ„ÙŠØ¯")
                        
                        # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± ÙˆØ§Ù„ØªØ­Ù‚Ù‚ (Ø¨Ø­Ø¯ Ø£Ù‚ØµÙ‰ 10 Ù…Ø­Ø§ÙˆÙ„Ø§Øª)
                        for attempt in range(10):
                            await asyncio.sleep(10)  # 10 Ø«ÙˆØ§Ù†ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª
                            
                            async with session.get(
                                f"{url}/{generation_id}",
                                headers=self.luma_headers
                            ) as check_response:
                                if check_response.status == 200:
                                    status_data = await check_response.json()
                                    state = status_data.get("state")
                                    
                                    if state == "completed":
                                        video_url = status_data.get("assets", {}).get("video")
                                        if video_url:
                                            return video_url
                                    elif state == "failed":
                                        failure_reason = status_data.get('failure_reason', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')
                                        raise Exception(f"ÙØ´Ù„ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {failure_reason}")
                                    elif state == "processing":
                                        continue  # Ø§Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±
                        
                        raise Exception("Ø§Ù†ØªÙ‡Ù‰ ÙˆÙ‚Øª Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù„ÙÙŠØ¯ÙŠÙˆ (100 Ø«Ø§Ù†ÙŠØ©)")
                    else:
                        error_text = await response.text()[:200]
                        raise Exception(f"Luma API error: {response.status} - {error_text}")
        except aiohttp.ClientError as e:
            raise Exception(f"Luma AI connection error: {str(e)}")
        except Exception as e:
            raise Exception(f"Luma AI error: {str(e)}")
    
    async def _generate_video_kling(self, prompt: str, image_url: str = None) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Kling AI"""
        # TODO: ØªÙ†ÙÙŠØ° API call Ù„Ù€ Kling AI
        # Ù…Ø¤Ù‚ØªØ§Ù‹ Ù†Ø³ØªØ®Ø¯Ù… fallback Ù„Ù€ Luma
        luma_config = self.providers[Provider.LUMA]
        if luma_config.enabled:
            return await self._generate_video_luma(prompt, image_url)
        raise Exception("Kling AI ØºÙŠØ± Ù…ØªÙˆÙØ± Ø­Ø§Ù„ÙŠØ§Ù‹")
    
    # ==================== Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© ====================
    
    def _clean_response(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø¯ÙˆØ¯"""
        if not text:
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£Ø³ØªØ·Ø¹ ØªÙƒÙˆÙŠÙ† Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨."
        
        try:
            clean_text = re.sub(r'THOUGHT:.*?(?=\n\n|\Z)', '', text, flags=re.DOTALL | re.IGNORECASE)
            clean_text = clean_text.replace("THOUGHT:", "").strip()
            
            if not clean_text or len(clean_text) < 2:
                return text
            return clean_text
        except:
            return text
    
    def check_user_limit(self, user_id: int, service_type: str) -> Tuple[bool, int]:
        """ÙØ­Øµ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙƒØ§Ø´ (LRU)
            if cache_key in self.user_limits_cache:
                current_usage = self.user_limits_cache[cache_key]
                # Ù†Ù‚Ù„ Ø§Ù„Ù…ÙØªØ§Ø­ Ù„Ù„Ù†Ù‡Ø§ÙŠØ© (Ø§Ù„Ø£Ø­Ø¯Ø«)
                self.user_limits_cache.move_to_end(cache_key)
            else:
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute(
                        'SELECT usage_count FROM ai_usage WHERE user_id = ? AND service_type = ? AND usage_date = ?',
                        (user_id, service_type, today)
                    )
                    result = cursor.fetchone()
                    current_usage = result[0] if result else 0
                    self.user_limits_cache[cache_key] = current_usage
            
            # Ø§Ù„ØªØ­ÙƒÙ… Ø¨Ø­Ø¬Ù… Ø§Ù„ÙƒØ§Ø´
            if len(self.user_limits_cache) > self.max_cache_size:
                self.user_limits_cache.popitem(last=False)
            
            limits_config = {
                "ai_chat": int(os.getenv("DAILY_AI_LIMIT", "20")),
                "image_gen": int(os.getenv("DAILY_IMAGE_LIMIT", "5")),
                "video_gen": int(os.getenv("DAILY_VIDEO_LIMIT", "2"))
            }
            
            limit = limits_config.get(service_type, 20)
            
            if current_usage >= limit:
                return False, 0
            
            return True, limit - current_usage
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„Ø­Ø¯ÙˆØ¯: {e}", exc_info=True)
            return True, 999
    
    def update_user_usage(self, user_id: int, service_type: str) -> bool:
        """ØªØ­Ø¯ÙŠØ« Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            current = self.user_limits_cache.get(cache_key, 0)
            self.user_limits_cache[cache_key] = current + 1
            
            # ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            with self.db.get_connection() as conn:
                conn.execute('''
                INSERT INTO ai_usage (user_id, service_type, usage_date, usage_count)
                VALUES (?, ?, ?, 1)
                ON CONFLICT(user_id, service_type, usage_date) 
                DO UPDATE SET usage_count = usage_count + 1
                ''', (user_id, service_type, today))
                conn.commit()
            return True
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: {e}", exc_info=True)
            return False
    
    def get_available_services(self) -> Dict[str, bool]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø§Øª"""
        return {
            "chat": len(self.get_available_providers(ServiceType.CHAT)) > 0,
            "image_generation": len(self.get_available_providers(ServiceType.IMAGE)) > 0,
            "video_generation": len(self.get_available_providers(ServiceType.VIDEO)) > 0
        }
    
    def get_user_stats(self, user_id: int) -> Dict[str, int]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        stats = {}
        today = datetime.now().strftime('%Y-%m-%d')
        
        for service_type in ["ai_chat", "image_gen", "video_gen"]:
            cache_key = f"{user_id}_{today}_{service_type}"
            stats[service_type] = self.user_limits_cache.get(cache_key, 0)
        
        return stats
    
    def get_system_stats(self) -> Dict[str, Any]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
        stats = {
            "providers": {},
            "total_requests_today": 0,
            "total_errors_today": 0,
            "discovery_completed": self.discovery_completed,
            "active_sessions": len(self.chat_sessions),
            "cache_size": len(self.user_limits_cache),
            "timestamp": datetime.now().isoformat()
        }
        
        for provider_name, config in self.providers.items():
            if config.enabled:
                stats["providers"][provider_name.value] = {
                    "enabled": True,
                    "usage_today": config.usage_today,
                    "errors_today": config.errors_today,
                    "daily_limit": config.daily_limit,
                    "remaining_limit": config.daily_limit - config.usage_today,
                    "last_error": config.last_error[:100] if config.last_error else None,
                    "active_models": config.active_models,
                    "discovered_models_count": {
                        st.value: len(models)
                        for st, models in config.discovered_models.items()
                    }
                }
                stats["total_requests_today"] += config.usage_today
                stats["total_errors_today"] += config.errors_today
        
        return stats
    
    def reset_daily_counts(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª"""
        today = datetime.now().strftime('%Y-%m-%d')
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† ÙƒØ§Ø´ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
        keys_to_delete = []
        for key in self.user_limits_cache.keys():
            if not key.endswith(today):
                keys_to_delete.append(key)
        
        for key in keys_to_delete:
            del self.user_limits_cache[key]
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ†
        for provider in self.providers.values():
            provider.usage_today = 0
            provider.errors_today = 0
            provider.last_error = None
        
        logger.info("ğŸ”„ ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©")

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù„Ù„ØªÙˆØ§ÙÙ‚
AIManager = SmartAIManager